import pandas as pd

data=pd.read_csv("/home/user/Artificaial Intelligence_JAN Batch/ML/Christy Sir//land_price.csv")

data.head()

data.describe()

data.dtypes

missing value
**************
#checking for missing value
data.isna().sum()

#showing columns with only missing values
missingval=data.isna().sum()[data.isna().sum()>0]
print(missingval)

#percentage of misisng
missing_val_percentage=data.isna().sum()[data.isna().sum()!=0]*100/len(data)
print(missing_val_percentage)

#columns with more than 40% missing
missing40cols=data.isna().sum()[data.isna().sum()*100/len(data)>40].index
missing40cols

#dropping columns with more than 40% missing
data=data.drop(missing40cols,axis=1)
data.shape

#filling missing value of object cols
#iterating over object columns and missing value is replaced with mode of the column
for x in missingobj:
    data[x][data[x].isna()] = data[x].mode()[0]

#filling missing value of float64 cols
#iterating over object columns and missing value is replaced with mean of the column
for x in missingfloat:
    data[x][data[x].isna()] = data[x].mean()

#categorical features managements : one-hot encoding
data=pd.get_dummies(data,drop_first=True)

creating X and y Datasets
*************************

#creating X and y Datasets
y=data['SalePrice']
X=data.drop(['Id','SalePrice'],axis=1)

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

PCA: reducing the dimensionality data without much lossing of information
*****
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(X)

pca.explained_variance_ratio_

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,200,1)
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')

#here we are diciding how many componets(columns) we have to take 
(np.cumsum(pca.explained_variance_ratio_))*100

#here we are decomposition our features to 20 coulmns. 
from sklearn.decomposition import PCA 
sklearn_pca = PCA(n_components=20)
X_PCA=sklearn_pca.fit_transform(X)

splitting X_train y_train
*************************
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_PCA, y, test_size=0.33,random_state=42) 
#0.67 data will be for training.

LinearRegression
*********************
from sklearn.linear_model import LinearRegression
reg = LinearRegression() #creating object of LinearRegression
reg.fit(X_train,y_train) #training and fitting LR object using training data
#fit command is used for training a model using train data.

ypred=reg.predict(X_test)  #predicting the saleprice for testing data
#in this context actual value will be y_test and predicted value wll ypred..
#error is calculated in such a way that vraiation of ypred from y_test

mean_squared_error
*********************
from sklearn.metrics import mean_squared_error #calculating MSE
testingerror=mean_squared_error(ypred,y_test)

Mean square error (MSE) is the average of the square of the errors. The larger the number the larger the error. Error in this case means the difference between the observed values y1, y2, y3, … and the predicted ones pred(y1), pred(y2), pred(y3), … 

r2_score
**********
from sklearn.metrics import r2_score
r2_score(y_test,ypred)

coef_
******
reg.coef_ #value for m1,m2,....mn

reg.intercept_ #value for c





				
